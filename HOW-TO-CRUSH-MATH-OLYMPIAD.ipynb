{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d8b17b",
   "metadata": {},
   "source": [
    "# HOW TO CRUSH MATH OLYMPIAD: PHUC Forecast Orchestration (SWE Pattern)\n",
    "\n",
    "This notebook refactors the math flow to match the same core pattern as `HOW-TO-CRUSH-SWE-BENCHMARK.ipynb`:\n",
    "\n",
    "1. `DREAM` (Scout)\n",
    "2. `FORECAST` (Grace)\n",
    "3. `DECIDE` (Judge)\n",
    "4. `ACT` (Solver)\n",
    "5. `VERIFY` (Skeptic)\n",
    "\n",
    "Key contract:\n",
    "- Use real remote Ollama model (`llama3.1:8b`) for the LLM lane.\n",
    "- Keep tool-assisted lane transparent and receipt-backed.\n",
    "- Produce machine-parseable artifacts per phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95161840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:41:00.881911Z",
     "iopub.status.busy": "2026-02-19T21:41:00.881236Z",
     "iopub.status.idle": "2026-02-19T21:41:00.890184Z",
     "shell.execute_reply": "2026-02-19T21:41:00.889127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHUC MATH ORCHESTRATION SETUP\n",
      "================================================================================\n",
      "run_id: imo-phuc-20260219T214100Z\n",
      "out_dir: /home/phuc/projects/stillwater/artifacts/notebook_imo_phuc/imo-phuc-20260219T214100Z\n",
      "model: llama3.1:8b\n",
      "url: http://192.168.68.100:11434\n",
      "timeout: 45.0\n",
      "pythonpath(cli/src): /home/phuc/projects/stillwater/cli/src\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "PYTHON = sys.executable\n",
    "CLI_MODULE = [PYTHON, \"-m\", \"stillwater\"]\n",
    "\n",
    "DEFAULT_MODEL = os.environ.get(\"STILLWATER_IMO_MODEL\", \"llama3.1:8b\")\n",
    "DEFAULT_URL = os.environ.get(\"STILLWATER_OLLAMA_URL\", \"http://192.168.68.100:11434\")\n",
    "DEFAULT_TIMEOUT = float(os.environ.get(\"STILLWATER_IMO_TIMEOUT\", \"45\"))\n",
    "\n",
    "RUN_ID = \"imo-phuc-\" + datetime.now(tz=timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "OUT_DIR = ROOT / \"artifacts\" / \"notebook_imo_phuc\" / RUN_ID\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLI_ENV = dict(os.environ)\n",
    "cli_src = str(ROOT / \"cli\" / \"src\")\n",
    "old_pp = CLI_ENV.get(\"PYTHONPATH\", \"\")\n",
    "CLI_ENV[\"PYTHONPATH\"] = cli_src if not old_pp else f\"{cli_src}{os.pathsep}{old_pp}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHUC MATH ORCHESTRATION SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print(\"run_id:\", RUN_ID)\n",
    "print(\"out_dir:\", OUT_DIR)\n",
    "print(\"model:\", DEFAULT_MODEL)\n",
    "print(\"url:\", DEFAULT_URL)\n",
    "print(\"timeout:\", DEFAULT_TIMEOUT)\n",
    "print(\"pythonpath(cli/src):\", cli_src)\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd1e0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:41:00.891295Z",
     "iopub.status.busy": "2026-02-19T21:41:00.891144Z",
     "iopub.status.idle": "2026-02-19T21:41:00.894175Z",
     "shell.execute_reply": "2026-02-19T21:41:00.893872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 IMO cases from /home/phuc/projects/stillwater/cli/tests/math/imo_qa_cases.json\n",
      "- P1: needle=2^(\n",
      "- P2: needle=empty\n",
      "- P3: needle=property holds\n",
      "- P4: needle=∠YPX + ∠KIL = 180\n",
      "- P5: needle=monochromatic triangle\n",
      "- P6: needle=f(x)=x\n"
     ]
    }
   ],
   "source": [
    "CASES_FILE = ROOT / \"cli\" / \"tests\" / \"math\" / \"imo_qa_cases.json\"\n",
    "if not CASES_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Missing cases file: {CASES_FILE}\")\n",
    "\n",
    "payload = json.loads(CASES_FILE.read_text(encoding=\"utf-8\"))\n",
    "CASES = []\n",
    "for row in payload.get(\"cases\", []):\n",
    "    if not isinstance(row, dict):\n",
    "        continue\n",
    "    cid = str(row.get(\"id\", \"\")).strip()\n",
    "    prompt = str(row.get(\"prompt\", \"\")).strip()\n",
    "    needle = str(row.get(\"needle\", \"\")).strip()\n",
    "    if cid and prompt and needle:\n",
    "        CASES.append({\"id\": cid, \"prompt\": prompt, \"needle\": needle})\n",
    "\n",
    "if not CASES:\n",
    "    raise RuntimeError(\"No valid IMO cases loaded.\")\n",
    "\n",
    "print(f\"Loaded {len(CASES)} IMO cases from {CASES_FILE}\")\n",
    "for c in CASES:\n",
    "    print(f\"- {c['id']}: needle={c['needle']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd05fb",
   "metadata": {},
   "source": [
    "## Phase Implementations (SWE-Pattern Mapping)\n",
    "\n",
    "Each phase emits a strict artifact:\n",
    "- `SCOUT_REPORT.json`\n",
    "- `FORECAST_MEMO.json`\n",
    "- `DECISION_RECORD.json`\n",
    "- `ACT_RESULT.json`\n",
    "- `SKEPTIC_VERDICT.json`\n",
    "\n",
    "The `ACT` phase runs two lanes for transparency:\n",
    "- `tool_assisted` (normal `twin` route)\n",
    "- `llm_only` (`twin --llm-only`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9b0aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:41:00.895284Z",
     "iopub.status.busy": "2026-02-19T21:41:00.895187Z",
     "iopub.status.idle": "2026-02-19T21:41:00.934350Z",
     "shell.execute_reply": "2026-02-19T21:41:00.933913Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_json(path: Path, data: Dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(data, indent=2, sort_keys=True) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def run_twin(prompt: str, *, llm_only: bool, model: str, url: str, timeout: float) -> Dict[str, Any]:\n",
    "    cmd = list(CLI_MODULE) + [\n",
    "        \"twin\",\n",
    "        prompt,\n",
    "        \"--model\",\n",
    "        model,\n",
    "        \"--url\",\n",
    "        url,\n",
    "        \"--timeout\",\n",
    "        str(timeout),\n",
    "        \"--json\",\n",
    "    ]\n",
    "    if llm_only:\n",
    "        cmd.append(\"--llm-only\")\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=str(ROOT),\n",
    "        env=CLI_ENV,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        check=False,\n",
    "    )\n",
    "\n",
    "    row: Dict[str, Any] = {\n",
    "        \"cmd\": cmd,\n",
    "        \"returncode\": proc.returncode,\n",
    "        \"stdout\": proc.stdout,\n",
    "        \"stderr\": proc.stderr,\n",
    "        \"llm_only\": llm_only,\n",
    "    }\n",
    "    if proc.returncode != 0:\n",
    "        row[\"ok\"] = False\n",
    "        return row\n",
    "\n",
    "    try:\n",
    "        payload = json.loads(proc.stdout)\n",
    "    except Exception as ex:\n",
    "        row[\"ok\"] = False\n",
    "        row[\"error\"] = f\"json_parse_error: {ex}\"\n",
    "        return row\n",
    "\n",
    "    row[\"ok\"] = True\n",
    "    row[\"payload\"] = payload\n",
    "    return row\n",
    "\n",
    "\n",
    "def scout_analyze(case: Dict[str, str]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"task_summary\": f\"Solve {case['id']} via PHUC orchestration with replayable receipts.\",\n",
    "        \"repro_command\": \"python -m stillwater twin <prompt> --json\",\n",
    "        \"failing_tests_or_errors\": [\n",
    "            \"Needle mismatch against expected marker\",\n",
    "            \"Wrong route action/source\",\n",
    "            \"Non-zero twin return code\",\n",
    "        ],\n",
    "        \"suspect_files_ranked\": [\n",
    "            {\"path\": \"cli/src/stillwater/cli.py\", \"reason\": \"routing + orchestration execution\"},\n",
    "            {\"path\": \"cli/settings/SWARM-ORCHESTRATION.prime-mermaid.md\", \"reason\": \"externalized tool policy\"},\n",
    "            {\"path\": \"imo/src/imo_2024_solver_proper.py\", \"reason\": \"deterministic benchmark evidence\"},\n",
    "        ],\n",
    "        \"witness_slices\": [\n",
    "            {\"path\": \"cli/src/stillwater/cli.py\", \"line_start\": 745, \"line_end\": 980},\n",
    "            {\"path\": \"cli/src/stillwater/cli.py\", \"line_start\": 1341, \"line_end\": 1880},\n",
    "        ],\n",
    "        \"acceptance_criteria\": [\n",
    "            \"tool_assisted response contains case needle\",\n",
    "            \"tool_assisted route action is phuc_swarms_benchmark\",\n",
    "            \"tool_assisted source is CPU\",\n",
    "            \"llm_only lane runs and is logged\",\n",
    "        ],\n",
    "        \"missing_assets\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def grace_forecast(case: Dict[str, str], scout_report: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"top_failure_modes_ranked\": [\n",
    "            {\"rank\": 1, \"mode\": \"route falls back to LLM unexpectedly\", \"likelihood_bucket\": \"30\"},\n",
    "            {\"rank\": 2, \"mode\": \"deterministic solver output format drifts\", \"likelihood_bucket\": \"30\"},\n",
    "            {\"rank\": 3, \"mode\": \"needle matcher is too weak/strict\", \"likelihood_bucket\": \"10\"},\n",
    "            {\"rank\": 4, \"mode\": \"remote ollama timeout\", \"likelihood_bucket\": \"30\"},\n",
    "        ],\n",
    "        \"edge_cases_to_test\": [\n",
    "            \"prompt variant with equivalent wording\",\n",
    "            \"historical IMO year prompt\",\n",
    "            \"model unavailable at configured URL\",\n",
    "        ],\n",
    "        \"compat_risks\": [\n",
    "            \"Changes in route.action semantics\",\n",
    "            \"Changes in response text that preserve meaning but break substring checks\",\n",
    "        ],\n",
    "        \"stop_rules\": [\n",
    "            \"If tool_assisted returncode != 0, mark FAIL\",\n",
    "            \"If tool_assisted action != phuc_swarms_benchmark, mark FAIL\",\n",
    "            \"If tool_assisted needle check fails, mark FAIL\",\n",
    "        ],\n",
    "        \"mitigations\": [\n",
    "            \"Capture per-lane stdout/stderr receipts\",\n",
    "            \"Log route metadata and phuc_decision for every case\",\n",
    "            \"Keep llm_only lane as public baseline\",\n",
    "        ],\n",
    "        \"case_id\": case[\"id\"],\n",
    "        \"scout_ref\": bool(scout_report),\n",
    "    }\n",
    "\n",
    "\n",
    "def judge_decide(case: Dict[str, str], scout_report: Dict[str, Any], forecast_memo: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"chosen_approach\": \"Run tool_assisted lane + llm_only lane, then verify strict route and needle checks.\",\n",
    "        \"alternatives_considered\": [\n",
    "            \"llm_only-only run\",\n",
    "            \"deterministic solver-only run without LLM baseline\",\n",
    "        ],\n",
    "        \"scope_locked\": [\"tool_assisted\", \"llm_only\", \"route_receipts\", \"needle_checks\"],\n",
    "        \"required_verification_rung\": 641,\n",
    "        \"required_tests\": [\n",
    "            \"tool_assisted route.action == phuc_swarms_benchmark\",\n",
    "            \"tool_assisted source == CPU\",\n",
    "            \"tool_assisted response contains expected needle\",\n",
    "            \"llm_only lane executed\",\n",
    "        ],\n",
    "        \"stop_rules\": forecast_memo.get(\"stop_rules\", []),\n",
    "        \"go_no_go_initial\": \"GO\",\n",
    "        \"case_id\": case[\"id\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def act_solver(case: Dict[str, str], decision_record: Dict[str, Any], *, model: str, url: str, timeout: float) -> Dict[str, Any]:\n",
    "    tool_run = run_twin(case[\"prompt\"], llm_only=False, model=model, url=url, timeout=timeout)\n",
    "    llm_run = run_twin(case[\"prompt\"], llm_only=True, model=model, url=url, timeout=timeout)\n",
    "    return {\n",
    "        \"case_id\": case[\"id\"],\n",
    "        \"decision_record_ref\": bool(decision_record),\n",
    "        \"tool_assisted\": tool_run,\n",
    "        \"llm_only\": llm_run,\n",
    "        \"model\": model,\n",
    "        \"url\": url,\n",
    "        \"timeout\": timeout,\n",
    "    }\n",
    "\n",
    "\n",
    "def skeptic_verify(case: Dict[str, str], act_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    needle = case[\"needle\"].lower()\n",
    "\n",
    "    def _lane_eval(run: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        out: Dict[str, Any] = {\n",
    "            \"run_ok\": bool(run.get(\"ok\")),\n",
    "            \"returncode\": run.get(\"returncode\"),\n",
    "            \"match\": False,\n",
    "            \"source\": None,\n",
    "            \"action\": None,\n",
    "            \"profile\": None,\n",
    "        }\n",
    "        payload = run.get(\"payload\", {}) if isinstance(run.get(\"payload\"), dict) else {}\n",
    "        route = payload.get(\"route\", {}) if isinstance(payload.get(\"route\"), dict) else {}\n",
    "        response = str(payload.get(\"response\", \"\"))\n",
    "        out[\"source\"] = payload.get(\"source\")\n",
    "        out[\"action\"] = route.get(\"action\")\n",
    "        phuc = route.get(\"phuc_decision\", {}) if isinstance(route.get(\"phuc_decision\"), dict) else {}\n",
    "        out[\"profile\"] = phuc.get(\"profile\")\n",
    "        out[\"match\"] = needle in response.lower()\n",
    "        out[\"response_excerpt\"] = response[:260]\n",
    "        out[\"route\"] = route\n",
    "        return out\n",
    "\n",
    "    tool_eval = _lane_eval(act_result[\"tool_assisted\"])\n",
    "    llm_eval = _lane_eval(act_result[\"llm_only\"])\n",
    "\n",
    "    tool_pass = bool(\n",
    "        tool_eval[\"run_ok\"]\n",
    "        and tool_eval[\"source\"] == \"CPU\"\n",
    "        and tool_eval[\"action\"] == \"phuc_swarms_benchmark\"\n",
    "        and tool_eval[\"match\"]\n",
    "    )\n",
    "\n",
    "    status = \"PASS\" if tool_pass else \"FAIL\"\n",
    "    reasons = []\n",
    "    if not tool_eval[\"run_ok\"]:\n",
    "        reasons.append(\"tool_assisted run failed\")\n",
    "    if tool_eval[\"source\"] != \"CPU\":\n",
    "        reasons.append(f\"unexpected source: {tool_eval['source']}\")\n",
    "    if tool_eval[\"action\"] != \"phuc_swarms_benchmark\":\n",
    "        reasons.append(f\"unexpected action: {tool_eval['action']}\")\n",
    "    if not tool_eval[\"match\"]:\n",
    "        reasons.append(\"needle mismatch\")\n",
    "\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"rung_achieved\": 641 if status == \"PASS\" else 0,\n",
    "        \"fail_reasons\": reasons,\n",
    "        \"required_fixes\": [\n",
    "            \"align tool route policy in SWARM-ORCHESTRATION\",\n",
    "            \"improve prompt-to-problem mapping for solver parser\",\n",
    "            \"improve deterministic answer extraction\",\n",
    "        ] if reasons else [],\n",
    "        \"tool_assisted\": tool_eval,\n",
    "        \"llm_only\": llm_eval,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45df330a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:41:00.935586Z",
     "iopub.status.busy": "2026-02-19T21:41:00.935478Z",
     "iopub.status.idle": "2026-02-19T21:41:00.940990Z",
     "shell.execute_reply": "2026-02-19T21:41:00.940593Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_case_pipeline(case: Dict[str, str], *, model: str, url: str, timeout: float) -> Dict[str, Any]:\n",
    "    case_dir = OUT_DIR / case[\"id\"]\n",
    "    case_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    scout = scout_analyze(case)\n",
    "    write_json(case_dir / \"SCOUT_REPORT.json\", scout)\n",
    "\n",
    "    forecast = grace_forecast(case, scout)\n",
    "    write_json(case_dir / \"FORECAST_MEMO.json\", forecast)\n",
    "\n",
    "    decide = judge_decide(case, scout, forecast)\n",
    "    write_json(case_dir / \"DECISION_RECORD.json\", decide)\n",
    "\n",
    "    act = act_solver(case, decide, model=model, url=url, timeout=timeout)\n",
    "    write_json(case_dir / \"ACT_RESULT.json\", act)\n",
    "\n",
    "    verify = skeptic_verify(case, act)\n",
    "    write_json(case_dir / \"SKEPTIC_VERDICT.json\", verify)\n",
    "\n",
    "    summary = {\n",
    "        \"case_id\": case[\"id\"],\n",
    "        \"status\": verify[\"status\"],\n",
    "        \"tool_match\": verify[\"tool_assisted\"][\"match\"],\n",
    "        \"tool_source\": verify[\"tool_assisted\"][\"source\"],\n",
    "        \"tool_action\": verify[\"tool_assisted\"][\"action\"],\n",
    "        \"llm_match\": verify[\"llm_only\"][\"match\"],\n",
    "        \"artifact_dir\": str(case_dir),\n",
    "    }\n",
    "    write_json(case_dir / \"SUMMARY.json\", summary)\n",
    "    return {\n",
    "        \"scout\": scout,\n",
    "        \"forecast\": forecast,\n",
    "        \"decide\": decide,\n",
    "        \"act\": act,\n",
    "        \"verify\": verify,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_full_pipeline(cases, *, model: str, url: str, timeout: float) -> Dict[str, Any]:\n",
    "    report: Dict[str, Any] = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"timestamp_utc\": datetime.now(tz=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"model\": model,\n",
    "        \"url\": url,\n",
    "        \"timeout\": timeout,\n",
    "        \"total_cases\": len(cases),\n",
    "        \"rows\": [],\n",
    "    }\n",
    "\n",
    "    for case in cases:\n",
    "        row = run_case_pipeline(case, model=model, url=url, timeout=timeout)\n",
    "        report[\"rows\"].append(row[\"summary\"])\n",
    "\n",
    "    tool_score = sum(1 for row in report[\"rows\"] if row[\"status\"] == \"PASS\")\n",
    "    llm_score = sum(1 for row in report[\"rows\"] if row[\"llm_match\"])\n",
    "\n",
    "    report[\"tool_assisted_score\"] = tool_score\n",
    "    report[\"llm_only_score\"] = llm_score\n",
    "    report[\"strict_pass\"] = tool_score == report[\"total_cases\"]\n",
    "    report[\"lane_disclosure\"] = {\n",
    "        \"tool_assisted\": \"PHUC swarms deterministic/tool route with receipts\",\n",
    "        \"llm_only\": \"remote Ollama model baseline via --llm-only\",\n",
    "    }\n",
    "\n",
    "    write_json(OUT_DIR / \"REPORT.json\", report)\n",
    "\n",
    "    md_lines = [\n",
    "        \"# PHUC IMO Pipeline Report\",\n",
    "        \"\",\n",
    "        f\"- run_id: `{RUN_ID}`\",\n",
    "        f\"- model: `{model}`\",\n",
    "        f\"- url: `{url}`\",\n",
    "        f\"- total_cases: `{report['total_cases']}`\",\n",
    "        f\"- tool_assisted_score: **{tool_score}/{report['total_cases']}**\",\n",
    "        f\"- llm_only_score: **{llm_score}/{report['total_cases']}**\",\n",
    "        f\"- strict_pass: `{report['strict_pass']}`\",\n",
    "        \"\",\n",
    "        \"## Per Case\",\n",
    "        \"\",\n",
    "    ]\n",
    "    for row in report[\"rows\"]:\n",
    "        md_lines.append(\n",
    "            f\"- {row['case_id']}: status={row['status']} tool(source={row['tool_source']}, action={row['tool_action']}, match={row['tool_match']}) llm_match={row['llm_match']}\"\n",
    "        )\n",
    "    md_lines.append(\"\")\n",
    "    md_lines.append(\"## Receipts\")\n",
    "    md_lines.append(\"\")\n",
    "    md_lines.append(f\"- report: `{OUT_DIR / 'REPORT.json'}`\")\n",
    "    md_lines.append(f\"- per-case artifacts: `{OUT_DIR}`\")\n",
    "    (OUT_DIR / \"REPORT.md\").write_text(\"\\n\".join(md_lines) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23711e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:41:00.942082Z",
     "iopub.status.busy": "2026-02-19T21:41:00.941988Z",
     "iopub.status.idle": "2026-02-19T21:41:42.863419Z",
     "shell.execute_reply": "2026-02-19T21:41:42.862873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL SCOREBOARD\n",
      "================================================================================\n",
      "tool_assisted: 6/6\n",
      "llm_only: 1/6\n",
      "strict_pass: True\n",
      "report_json: /home/phuc/projects/stillwater/artifacts/notebook_imo_phuc/imo-phuc-20260219T214100Z/REPORT.json\n",
      "report_md: /home/phuc/projects/stillwater/artifacts/notebook_imo_phuc/imo-phuc-20260219T214100Z/REPORT.md\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "report = run_full_pipeline(CASES, model=DEFAULT_MODEL, url=DEFAULT_URL, timeout=DEFAULT_TIMEOUT)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL SCOREBOARD\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"tool_assisted: {report['tool_assisted_score']}/{report['total_cases']}\")\n",
    "print(f\"llm_only: {report['llm_only_score']}/{report['total_cases']}\")\n",
    "print(f\"strict_pass: {report['strict_pass']}\")\n",
    "print(\"report_json:\", OUT_DIR / \"REPORT.json\")\n",
    "print(\"report_md:\", OUT_DIR / \"REPORT.md\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not report[\"strict_pass\"]:\n",
    "    raise AssertionError(\"Tool-assisted lane did not reach full score. Inspect REPORT.json receipts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44577e3",
   "metadata": {},
   "source": [
    "## Claim Hygiene\n",
    "\n",
    "This notebook is now symmetric with SWE-style orchestration structure, but still math-specific in verifier logic.\n",
    "\n",
    "- `tool_assisted` lane score is a workflow score over configured IMO cases.\n",
    "- `llm_only` is baseline model behavior on the same prompts.\n",
    "- This is not official IMO grading and not a formal proof certificate.\n",
    "\n",
    "Next step after this notebook is stable: port identical phase contracts to CLI command paths.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
