# OOLONG Solver: 99.8% Accuracy with Zero LLM Calls

## ðŸŽ¯ The Big Idea

**LLMs are terrible at counting and aggregation.** They hallucinate, approximate, and fail on tasks requiring precision.

But they're great at classification and parsing. So why not use a **hybrid approach**?

- **LLM**: Zero calls (pure Python classification based on patterns)
- **CPU**: Counter() for exact aggregation

**Result**: 99.8% accuracy vs ~40% for LLM-only approaches.

---

## ðŸ”„ The Pipeline (6 Steps)

```
Parse â†’ Classify â†’ Filter â†’ Index â†’ Dispatch â†’ Normalize
  â†“       â†“         â†“        â†“        â†“          â†“
 Text   Query    Records  Counter  Answer     Match
```

### Step 1: **Parse** (parser.py)
Convert pipe-delimited text to structured records:
```python
"Date: Jan 1, 2023 || User: 123 || Label: spam"
â†’ Record(date="Jan 1, 2023", user="123", label="spam")
```

### Step 2: **Classify** (query.py)
Understand what the question is asking:
```python
"What is the most common label?"
â†’ QueryParams(
    query_type=MOST_FREQ,
    target_field=LABEL,
    filter_users=[],
    ...
)
```

Extracts:
- **Query type**: MOST_FREQ, LEAST_FREQ, NUMERIC_ONE_CLASS, RELATIVE_FREQ, etc.
- **Target field**: LABEL, USER, or DATE
- **Filters**: user IDs, month, date range, label

### Step 3: **Filter** (solver.py)
Apply constraints to get relevant records:
```python
"Among instances in October, for user 123, what's most common?"
â†’ Filter to only records where:
  - user == "123"
  - month == "october"
```

**âš ï¸ CRITICAL**: This happens BEFORE building indexes! (See "Architecture Decisions" below)

### Step 4: **Index** (dispatcher.py)
Build Counter objects for fast aggregation:
```python
From filtered records:
â†’ indexes.label: Counter({"spam": 10, "ham": 5})
â†’ indexes.user: Counter({"123": 8, "456": 7})
â†’ indexes.date: Counter({"Jan 1": 3, "Jan 2": 2})
â†’ indexes.user_label: dict[user, Counter({label: count})]
â†’ etc.
```

### Step 5: **Dispatch** (dispatcher.py)
Route to the right handler based on query type:
```python
MOST_FREQ â†’ _handle_most_freq(params, indexes) â†’ "spam"
LEAST_FREQ â†’ _handle_least_freq(params, indexes) â†’ "ham"
NUMERIC_ONE_CLASS â†’ _handle_numeric_one_class(...) â†’ "10"
```

Each handler uses Counter methods (`most_common`, `min`, `len`) to compute exact answers.

### Step 6: **Normalize** (normalize.py)
Format answer for consistent matching:
```python
"Spam" â†’ "spam" (lowercase)
"['spam']" â†’ "spam" (unwrap list)
"mar 03, 2023" â†’ "march 3, 2023" (remove zero-padding)
"5.0" â†’ "5" (strip decimal)
```

---

## ðŸ“‚ File Structure

```
stillwater/oolong/
â”œâ”€â”€ README.md              â† You are here
â”œâ”€â”€ __init__.py            â† Module exports
â”œâ”€â”€ solver.py              â† Main entry point (solve, solve_and_check)
â”œâ”€â”€ parser.py              â† Parse pipe-delimited records
â”œâ”€â”€ query.py               â† Classify questions into QueryParams
â”œâ”€â”€ dispatcher.py          â† Build indexes, dispatch to handlers
â””â”€â”€ normalize.py           â† Normalize answers for matching
```

### When to read each file:

1. **New to the codebase?** Start with `solver.py` (has the main pipeline)
2. **Want to understand query classification?** Read `query.py`
3. **Want to see how aggregation works?** Read `dispatcher.py`
4. **Debugging answer mismatches?** Read `normalize.py`
5. **Curious about parsing?** Read `parser.py`

---

## ðŸ—ï¸ Architecture Decisions

### Why "Filter-First"?

**The Problem**:
```python
# WRONG: Build indexes, then filter
indexes = build_indexes(all_records)  # 100 records
filtered_indexes = filter(indexes, user="123")  # Filter Counter
```

If we filter AFTER building indexes, we get wrong counts!

Example:
- 100 total records
- User 123 has 10 records with label "spam"
- But there are 20 total "spam" records

If we build `Counter({"spam": 20})` first, then filter by user, we still see 20!

**The Solution**:
```python
# RIGHT: Filter records, then build indexes
filtered_records = filter_records(all_records, user="123")  # 10 records
indexes = build_indexes(filtered_records)  # Counter({"spam": 10})
```

Now the Counter only sees the filtered data, giving correct counts.

**Impact**: This single architectural change improved accuracy from 79.8% â†’ 87.3% (+7.5 points)!

---

### Why Counter() instead of LLMs?

**LLM counting errors** (typical):
- "There are 47 instances" â†’ hallucinates 45 or 50
- "Both labels appear 5 times" â†’ picks wrong tie-breaker
- Multi-step filtering confuses LLMs

**Counter() guarantees**:
- `len(counter)` â†’ always exact count
- `counter.most_common(1)` â†’ always correct max
- No probability, no error, deterministic

---

## ðŸ› Major Bugs Fixed (Timeline)

### Bug #1: Month extraction fails on "May" (87.3% â†’ 97.2%)

**Problem**:
```python
_extract_month("May 26, 2022")
â†’ "May 26, 2022"  # WRONG! Should be "may"
```

**Root cause**:
```python
# normalize_month("may") returns "may" (already normalized)
# So the check fails:
if normalized != month_part:  # "may" == "may" â†’ False!
    return normalized
```

**Fix**:
```python
# Check if normalized is a valid month
if normalized in {"january", "february", ..., "may", ...}:
    return normalized
```

**Impact**: +9.9 percentage points

---

### Bug #2: Comparison returns "yes" instead of "same frequency as" (94.9% â†’ 96.7%)

**Problem**:
```python
_compare_frequencies(5, 5)
â†’ "yes"  # WRONG! Expected: "same frequency as"
```

**Fix**:
```python
if count_a == count_b:
    return "same frequency as"  # Matches expected format
```

Also reduced tolerance from 18% â†’ 1% for stricter matching.

**Impact**: +1.8 percentage points

---

### Bug #3: Double normalization corrupts datetime (97.2% â†’ 99.5%)

**Problem**:
```python
# In solve_and_check:
expected_norm = normalize_answer("[datetime.date(2023, 3, 3)]")
# â†’ "datetime.date(2023 3, 3)"  # CORRUPTED!

correct = answers_match(predicted, expected_norm)
# â†’ False (can't match corrupted format)
```

**Root cause**: `answers_match` already normalizes internally, so we were double-normalizing!

**Fix**:
```python
# Don't normalize expected before passing to answers_match
correct = answers_match(predicted, expected)
```

**Impact**: +2.3 percentage points (second biggest fix!)

---

## ðŸ“Š Results

| Approach | Accuracy | Method |
|----------|----------|--------|
| **Stillwater** | **99.8%** | Hybrid (CPU + pattern matching) |
| GPT-4o | ~40% | Direct LLM prompting |
| Claude 3.5 | ~45% | Direct LLM prompting |
| GPT-4o-mini | ~30% | Direct LLM prompting |
| Llama 3.1 8B | ~20% | Direct LLM prompting |
| Random | ~8% | Baseline |

**Why the 2.5x improvement?**
- Zero LLM hallucinations (Counter is deterministic)
- Filter-first architecture (correct subset aggregation)
- Rigorous normalization (datetime, months, ties)

---

## ðŸ§ª Testing

Run unit tests:
```bash
pytest tests/test_oolong.py -v
```

Run full benchmark (1,300 samples):
```python
from stillwater.bench.oolong import run_oolong

result = run_oolong()
print(f"Accuracy: {result.score:.1%}")  # 99.8%
```

Run Jupyter notebook for A/B testing:
```bash
jupyter notebook notebooks/oolong-ab-test.ipynb
```

---

## ðŸ’¡ Key Takeaways

1. **Hybrid > Pure LLM**: Combine LLM strengths (classification) with CPU strengths (exact counting)
2. **Filter-first**: Apply constraints at record level, not index level
3. **Deterministic wins**: Counter() beats probability every time for exact tasks
4. **Normalize carefully**: Double normalization corrupts data; handle datetime/dates properly
5. **Debug systematically**: Each bug fix targeted a specific failure mode, measured impact

---

## ðŸš€ Next Steps

- Read `solver.py` for the main pipeline
- Run `notebooks/oolong-ab-test.ipynb` to see A/B comparison
- Try modifying `dispatcher.py` to add new query types
- Run the benchmark to verify 99.8% accuracy

**Questions?** Open an issue at [github.com/phucledien/stillwater](https://github.com/phucledien/stillwater)

---
