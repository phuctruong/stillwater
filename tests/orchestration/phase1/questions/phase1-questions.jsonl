{"id": "PH1-Q-001", "text": "What is the time complexity of extract_keywords()? The regex findall is O(n) where n = len(text), the stop_word filtering is O(m) where m = word count, and dedup is O(m). Total: O(n + m). Is this documented?", "persona": "Knuth", "domain": "algorithms", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-002", "text": "Is the stop_word lookup O(1) or O(n)? The code uses a set literal (line 77-89 of cpu_learner.py), so lookup is O(1) amortized. But is there a test that verifies this remains a set and not accidentally converted to a list?", "persona": "Knuth", "domain": "algorithms", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-003", "text": "What happens when CPULearner._patterns is empty? Does predict() return (None, 0.0, [])? Trace: keywords extracted, loop finds no matches, returns (None, 0.0, []). Is there a test for this cold-start case?", "persona": "Knuth", "domain": "algorithms", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-004", "text": "Is the confidence formula 1 - 1/(1 + 0.3 * count) monotonically increasing? Yes: derivative d/d(count) = 0.3/(1 + 0.3*count)^2 > 0 for all count >= 0. But is there a property-based test asserting monotonicity?", "persona": "Knuth", "domain": "algorithms", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-005", "text": "What is the maximum theoretical confidence? It approaches 1.0 asymptotically but never reaches it. At count=25 (the seed default) confidence is 0.8824. At count=100 it is 0.9677. Is there a test that confirms confidence < 1.0 for any finite count?", "persona": "Knuth", "domain": "algorithms", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-006", "text": "At what count does confidence cross each threshold? Phase 1 (0.70) needs count >= 8 (0.7059). Phase 2 (0.80) needs count >= 14 (0.8077). Phase 3 (0.90) needs count >= 30 (0.9000). The default seeds ship with count=25, which passes Phase 1 and Phase 2 but NOT Phase 3. Is this intentional?", "persona": "Knuth", "domain": "algorithms", "category": "defaults", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-007", "text": "The deduplication in extract_keywords uses a seen-set + list. This preserves insertion order. But learn() does a SECOND dedup via dict.fromkeys(). Is the double-dedup intentional or redundant? The second dedup also covers reasoning keywords merged with input keywords.", "persona": "Knuth", "domain": "algorithms", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-008", "text": "Can adversarial input bypass Phase 1 classification? Craft a prompt that looks like a greeting but is actually a task: 'Hello there! Please fix the authentication bug in the OAuth module immediately.' With default seeds, both 'hello' (greeting) and 'fix' (task) match. Since all seeds have count=25 and identical confidence 0.8824, the FIRST keyword in extraction order wins. 'hello' comes first, so the label is 'greeting' — the task is misclassified.", "persona": "Schneier", "domain": "security", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-009", "text": "What if user injects newlines or unicode characters? The regex r'[a-z]+' on text.lower() only matches ASCII lowercase. Newlines are ignored (not [a-z]). Unicode letters like accented characters are silently dropped. Zero-width spaces split tokens but the regex skips them. This means 'fix\\u200Bbug' correctly extracts ['fix', 'bug']. Is this documented?", "persona": "Schneier", "domain": "security", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-010", "text": "Can Phase 1 be DoS'd with extremely long input? extract_keywords calls re.findall(r'[a-z]+', text.lower()). For a 1MB input of repeated 'a', this produces a single token of 1M characters, which survives stop-word filtering (not in set) and len >= 3. This single massive token would be stored in _patterns. Is there a max-input-length guard?", "persona": "Schneier", "domain": "security", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-011", "text": "What happens with null bytes in input? text.lower() preserves null bytes. re.findall(r'[a-z]+', ...) skips them (null is not [a-z]). So 'hello\\x00world' correctly produces ['hello', 'world']. But if null bytes appear in a JSONL persistence file, json.loads may handle them as \\u0000. Is there a test for round-trip with null bytes in examples?", "persona": "Schneier", "domain": "security", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-012", "text": "Is the keyword extraction vulnerable to ReDoS? The regex r'[a-z]+' is a simple character class with no nested quantifiers or alternation. It cannot cause catastrophic backtracking. VERDICT: Not vulnerable. But is this explicitly documented as a security property?", "persona": "Schneier", "domain": "security", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-013", "text": "What if someone adds a custom seed that overrides a default seed? The load() method iterates JSONL lines and sets self._patterns[kw] = {...} for each. If the same keyword appears twice, the LAST one wins (dict assignment). So a custom seed file loaded after defaults can silently override any default. Is there a conflict detection mechanism?", "persona": "Schneier", "domain": "security", "category": "customization", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-014", "text": "What is the precondition for CPULearner.predict()? The text parameter must be a string. What happens if None is passed? text.lower() would raise AttributeError. What about an integer? Same. There is no type check or guard clause at the top of predict(). Is this a contract violation or an acceptable caller responsibility?", "persona": "Liskov", "domain": "contracts", "category": "cpu_flow", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-015", "text": "What is the postcondition of predict()? Returns tuple of (label_or_None, float, list_of_strings). The type hint says tuple but does not specify the inner types. Is the return type documented as tuple[str | None, float, list[str]]? Does any caller rely on unpacking exactly 3 elements?", "persona": "Liskov", "domain": "contracts", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-016", "text": "What invariant must hold: confidence is always in [0.0, 1.0]? The formula 1 - 1/(1 + 0.3*count) with count >= 0 always produces [0.0, 1.0). But if count were negative (data corruption), the formula could produce values outside this range. Is there a clamp or assertion?", "persona": "Liskov", "domain": "contracts", "category": "invariants", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-017", "text": "Does the Phase 1 to Phase 2 handoff preserve all original text? When predict() returns, the caller gets (label, confidence, matched_keywords) but NOT the original text. The original text must be preserved by the caller. Is this contract documented? What if the caller only passes the label forward?", "persona": "Liskov", "domain": "contracts", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-018", "text": "What happens if Phase 1 returns 'task' with confidence 0.71 — barely above the 0.70 threshold? can_handle() uses >= comparison, so 0.70 exactly returns True. But is there a hysteresis band? If confidence fluctuates near the boundary on successive calls with slight input variations, the system oscillates between CPU and LLM paths.", "persona": "Liskov", "domain": "contracts", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-019", "text": "Is the threshold comparison > or >=? can_handle() on line 211 uses 'conf >= self.threshold'. So confidence of exactly 0.70 passes Phase 1. But predict() uses 'conf > best_conf' (strictly greater) for winner selection. These two different comparison operators are a subtle inconsistency. Is this intentional?", "persona": "Liskov", "domain": "contracts", "category": "cpu_flow", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-020", "text": "The learn() method has a side effect: entry['label'] = label uses last-label-wins semantics (line 140). If the same keyword is learned with different labels across calls, only the last label survives. This violates the principle of least surprise. Is there a test that demonstrates this overwrite behavior?", "persona": "Liskov", "domain": "contracts", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-021", "text": "What input FALSIFIES the claim 'Phase 1 correctly classifies all greetings'? Input: 'Hello! Fix the tests'. The keyword 'hello' maps to greeting, 'fix' maps to task, 'tests' maps to task (via seed 'test'? No — 'tests' != 'test'; the regex extracts 'tests' not 'test'). With default seeds only 'hello' and 'fix' match. Both have confidence 0.8824. Tie: 'hello' wins (first in order). Classification: 'greeting'. But the user's intent is clearly a task. FALSIFIED.", "persona": "Turing", "domain": "falsification", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-022", "text": "Can you construct a prompt that Phase 1 classifies as 'task' but should be 'greeting'? Input: 'Just wanted to say hello and help you feel welcome'. After stop-word filtering: ['wanted', 'say', 'hello', 'help', 'feel', 'welcome']. Only 'hello' (greeting) and 'help' (task) match seeds. Tie at 0.8824: 'hello' comes first, so greeting wins. Actually this case works correctly. Try: 'help me say hello' — 'help' comes first, wins as 'task'. But user intent is social. FALSIFIED.", "persona": "Turing", "domain": "falsification", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-023", "text": "What about a prompt that should be 'task' but Phase 1 blocks as 'greeting'? 'Hello, please deploy the new build to production'. Keywords: ['hello', 'deploy', 'build', 'production']. 'hello'->greeting (0.8824), 'deploy'->task (0.8824), 'build'->task (0.8824). Tie between hello and deploy: hello comes first, wins. Label: 'greeting'. Three task keywords are overridden by one greeting keyword due to first-match-wins tie-breaking. CRITICAL BUG.", "persona": "Turing", "domain": "falsification", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-024", "text": "'Hello! Fix the tests' — does the greeting seed kill a legitimate task? YES. With equal confidence, the greeting keyword 'hello' appears first in the text and wins the tie. The task keywords 'fix' (and potentially 'tests' if it were a seed) are silently ignored. The matched list will contain all tying keywords, but the label comes from the first one.", "persona": "Turing", "domain": "falsification", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-025", "text": "What if the ONLY keyword is a task keyword but it appears AFTER a greeting keyword? In the predict() algorithm, keyword order in the extracted list determines the winner on ties. The extraction preserves input text order. So 'hi there, please deploy' extracts ['please'->stop, 'deploy'->task]. Only 'deploy' matches. Label: 'task'. Correct. But 'hello deploy' extracts ['hello', 'deploy']. Tie: 'hello' wins. Incorrect.", "persona": "Turing", "domain": "falsification", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-026", "text": "Does order of keywords affect classification? YES. predict() iterates keywords in extraction order. On ties, the first keyword's label wins. This means 'deploy hello' classifies as 'task' but 'hello deploy' classifies as 'greeting'. Same words, different order, different classification. This is a position-dependent bias bug.", "persona": "Turing", "domain": "falsification", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-027", "text": "What loop invariant holds during keyword extraction? After processing the i-th word: (1) 'seen' contains exactly the unique non-stop words with len>=3 from words[0..i], (2) 'unique' contains them in first-occurrence order, (3) len(seen) == len(unique). Is this invariant tested?", "persona": "Dijkstra", "domain": "invariants", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-028", "text": "When no keywords survive stop_word filtering, what state does the system enter? extract_keywords returns []. predict() iterates an empty list, the loop body never executes, and returns (None, 0.0, []). can_handle() gets conf=0.0, returns False (0.0 < 0.70). The system falls through to LLM. Is this the correct degradation path?", "persona": "Dijkstra", "domain": "invariants", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-029", "text": "Is there a termination guarantee for predict()? The function iterates over a finite list (keywords) and does O(1) work per keyword (dict lookup + confidence calculation). No recursion, no while loops. Termination is guaranteed. But what about confidence() — could the cache lookup diverge? No: it is a simple dict get. Termination: PROVEN.", "persona": "Dijkstra", "domain": "invariants", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-030", "text": "What if the same keyword maps to two different labels? In learn(), line 140: entry['label'] = label overwrites unconditionally. So the last learn() call for a given keyword determines its label. The _patterns dict stores exactly one label per keyword. The mapping is a function (keyword -> label) but NOT injective (multiple keywords can map to the same label). Is the non-injective property tested?", "persona": "Dijkstra", "domain": "invariants", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-031", "text": "Is the seed-to-label mapping injective (one keyword maps to one label)? In the current seed file, yes: each keyword appears exactly once with one label. But the data model allows overwriting via load() or learn(). If two JSONL records have the same keyword but different labels, the second record's label wins. Is there a uniqueness constraint enforced at load time?", "persona": "Dijkstra", "domain": "invariants", "category": "defaults", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-032", "text": "What about homophones or homographs? 'lead' as in 'lead the team' (task-like) vs 'lead' as in 'lead poisoning' (not a task). The CPU learner treats all occurrences of 'lead' identically — it has no word sense disambiguation. Same for 'test' (run tests vs test of patience), 'run' (execute vs physical running). Is word sense ambiguity acknowledged in the design doc?", "persona": "Dijkstra", "domain": "invariants", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-033", "text": "What test is MISSING from the current 88 tests? There is no test for the tie-breaking behavior when two keywords with different labels have identical confidence. This is the most critical gap because it hides the position-dependent classification bug (PH1-Q-026).", "persona": "Beck", "domain": "tdd", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-034", "text": "Write a red test for the tie-breaking bug: def test_tie_breaking_greeting_vs_task(): learner = CPULearner('phase1'); learner.load('seeds.jsonl'); label, conf, matched = learner.predict('hello deploy'); assert label == 'task', f'Expected task but got {label} — greeting keyword won due to position bias'", "persona": "Beck", "domain": "tdd", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-035", "text": "Write a red test for empty input: def test_empty_input(): learner = CPULearner('phase1'); label, conf, matched = learner.predict(''); assert label is None; assert conf == 0.0; assert matched == []", "persona": "Beck", "domain": "tdd", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-036", "text": "Write a red test for all-stop-words input: def test_all_stop_words(): learner = CPULearner('phase1'); label, conf, matched = learner.predict('the is a an it in on at to for'); assert label is None; assert conf == 0.0; assert matched == []", "persona": "Beck", "domain": "tdd", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-037", "text": "Write a red test for mixed greeting+task input: def test_mixed_greeting_task(): learner = CPULearner('phase1'); learner.load('seeds.jsonl'); label, conf, matched = learner.predict('Hello! Please fix the authentication bug'); # BUG: 'hello' wins tie. This test documents the bug. assert label == 'greeting'  # Actual current behavior (should arguably be 'task')", "persona": "Beck", "domain": "tdd", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-038", "text": "What is the simplest change that would break 50% of seeds? Change the stop_words set to include 'fix'. This kills the most common task seed. Or: set MIN_FREQUENCY_FOR_CONFIDENCE to 100, which would make all count=25 seeds fail Phase 3 (0.90 threshold). Or: change the regex from r'[a-z]+' to r'[a-z]{4,}' — this kills 'fix', 'bug', 'run', 'add', 'sad' (all 3-letter seeds).", "persona": "Beck", "domain": "tdd", "category": "defaults", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-039", "text": "Are there property-based tests? For example: for any input string, confidence returned by predict() must be in [0.0, 1.0]. For any input, predict() must return a 3-tuple. For any input, matched_keywords must be a subset of extract_keywords(input). These property tests are absent from the test suite.", "persona": "Beck", "domain": "tdd", "category": "cpu_flow", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-040", "text": "What happens step by step in the CPU flow? Input text -> text.lower() -> re.findall(r'[a-z]+', ...) -> filter stop_words (set membership, O(1)) -> filter len >= 3 -> deduplicate preserving order -> for each keyword: lookup in _patterns dict -> compute confidence via formula -> track best (strictly greater wins) -> return (best_label, best_conf, matched_keywords). Is this pipeline documented end-to-end?", "persona": "Knuth", "domain": "algorithms", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-041", "text": "When exactly does the LLM get called? Only when can_handle() returns False (confidence below threshold). The LLM is NOT called from CPULearner itself — the caller (orchestration layer) decides. What model? What prompt template? What is the cost per LLM call? These are Phase 2+ questions but Phase 1 must define the handoff contract clearly.", "persona": "Knuth", "domain": "algorithms", "category": "llm_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-042", "text": "How does a user add a new label? They must: (1) create a JSONL record with the new keyword and label, (2) add it to the seeds file or a custom seeds file, (3) call learner.load() to ingest it. There is no CLI command like 'sw add-seed --keyword deploy --label devops'. Is the customization path documented?", "persona": "Beck", "domain": "tdd", "category": "customization", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-043", "text": "What ships with 'pip install stillwater'? The default seeds file at data/default/seeds/small-talk-seeds.jsonl with 49 seeds (43 task + 1 greeting + 1 gratitude + 1 emotional_positive + 1 emotional_negative + 1 humor + 1 small_talk). Plus the CPULearner class, the SmallTalkCPU class, and the IntentCPU class. Is the default file list documented in a manifest?", "persona": "Beck", "domain": "tdd", "category": "defaults", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-044", "text": "What metric proves Phase 1 works? CPU hit rate: percentage of inputs where can_handle() returns True (confidence >= 0.70). Classification accuracy: percentage where the CPU label matches the LLM label (requires Phase 2 feedback loop). Currently there is NO accuracy metric — only hit rate. Is hit rate alone sufficient as a northstar metric?", "persona": "Knuth", "domain": "algorithms", "category": "northstar", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-045", "text": "Does Phase 1 follow the SW5.0 5-layer architecture? Layer 1 (Data): seeds JSONL file. Layer 2 (CPU): CPULearner with extract_keywords + predict + confidence. Layer 3 (LLM): not in Phase 1 (handoff only). Layer 4 (Verification): can_handle threshold check. Layer 5 (Persistence): save/load JSONL. Is this mapping correct and is it documented?", "persona": "Dijkstra", "domain": "invariants", "category": "sw5_compliance", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-046", "text": "The confidence_cache is invalidated per-keyword in learn() (line 145). But what if predict() is called concurrently from another thread while learn() is modifying _patterns? The dict operations are not atomic. Is thread safety documented as a non-goal, or is there a lock?", "persona": "Schneier", "domain": "security", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-047", "text": "The learn() method truncates examples to 80 characters (line 142: input_text[:80]). What if the meaningful part of the input is at position 81+? This truncation loses evidence. Is 80 characters sufficient for all expected inputs? What about URLs or code snippets?", "persona": "Liskov", "domain": "contracts", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-048", "text": "In predict(), the matched list has a subtle bug: when conf == best_conf, the keyword is appended to matched, but the label is NOT updated. So matched contains keywords from DIFFERENT labels, but the returned label is only from the first winner. The caller cannot know that matched keywords disagree on labels. Should predict() return a conflict flag?", "persona": "Turing", "domain": "falsification", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-049", "text": "The seed file has all 49 seeds with identical count=25 and confidence=0.8824. This means EVERY seed has the same weight. The system has no ability to distinguish between strong signals (e.g., 'deploy' is almost always a task) and weak signals (e.g., 'data' could be small talk or task). Is uniform seeding a deliberate cold-start strategy or an oversight?", "persona": "Knuth", "domain": "algorithms", "category": "defaults", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-050", "text": "The extract_keywords regex r'[a-z]+' strips ALL non-ASCII characters. This means inputs in non-English languages produce ZERO keywords and always fall through to LLM. Is internationalization a Phase 1 non-goal, and is it documented?", "persona": "Schneier", "domain": "security", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-051", "text": "The word 'tests' (plural) does NOT match the seed 'test' (singular). The regex extracts exact tokens; there is no stemming or lemmatization. So 'run the tests' extracts ['run', 'tests'] and only 'run' matches a seed. 'tests' is a dead keyword. How many common plurals/verb forms are missing from the seed file?", "persona": "Turing", "domain": "falsification", "category": "magic_words", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-052", "text": "The Phase 1 intent CPU (IntentCPU in admin/orchestration/intent/cpu.py) has a DIFFERENT stop word set than CPULearner (src/cli/src/stillwater/cpu_learner.py). IntentCPU uses a frozenset of 70 words including 'help', 'make', 'get', 'set', 'use', 'want', 'need', 'like', 'well', 'hi', 'hey', 'ok', 'okay'. CPULearner does NOT filter these. This means 'help' is a valid seed keyword in CPULearner but is stopped in IntentCPU. Are these two components aware of each other's stop word lists?", "persona": "Dijkstra", "domain": "invariants", "category": "sw5_compliance", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-053", "text": "What happens when the seed file is missing or empty at startup? load() checks os.path.exists() and returns silently if the file does not exist (line 246-247). This is a 'cold start' no-op. But should it log a warning? The system will classify everything as (None, 0.0, []) until learn() is called.", "persona": "Beck", "domain": "tdd", "category": "defaults", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-054", "text": "The save() method creates directories with os.makedirs(exist_ok=True) but does NOT handle write permission errors. If the data directory is read-only (e.g., installed via pip into site-packages), save() will raise PermissionError. Is there a fallback path or user-writable default?", "persona": "Schneier", "domain": "security", "category": "customization", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-055", "text": "The stats() method computes avg_confidence over ALL patterns, not just those above threshold. This can be misleading: if 90% of patterns have low confidence, the average looks bad even though the high-confidence patterns work correctly. Should stats() separate above-threshold from below-threshold metrics?", "persona": "Beck", "domain": "tdd", "category": "northstar", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-056", "text": "Phase 1 has no concept of 'negative seeds' — keywords that should REDUCE confidence for a label. For example, 'joke' in the context 'this deployment is no joke' should reduce humor confidence. The system treats all keyword occurrences as positive evidence. Is negative evidence a Phase 2 feature?", "persona": "Turing", "domain": "falsification", "category": "cpu_flow", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-057", "text": "The SmallTalkCPU (admin/orchestration/smalltalk/cpu.py) and CPULearner (src/cli/src/stillwater/cpu_learner.py) are separate systems with separate stop word lists, separate keyword extraction, and separate classification logic. Which one is 'Phase 1'? Are they both Phase 1? Is there a clear boundary between them?", "persona": "Dijkstra", "domain": "invariants", "category": "sw5_compliance", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-058", "text": "Write a red test for keyword order sensitivity: def test_order_sensitivity(): learner = CPULearner('phase1'); learner.load('seeds.jsonl'); label1, _, _ = learner.predict('hello deploy'); label2, _, _ = learner.predict('deploy hello'); assert label1 == label2, f'Order-dependent: {label1} vs {label2}'  # This test will FAIL: hello-first gives greeting, deploy-first gives task", "persona": "Beck", "domain": "tdd", "category": "edge_cases", "severity": "HIGH", "status": "OPEN"}
{"id": "PH1-Q-059", "text": "The learn() method caps examples at 5 per keyword (line 141). But there is no way to rotate old examples out. Once 5 examples are stored, all subsequent examples for that keyword are silently dropped. Over time, the examples become stale. Is there a staleness detection or example rotation strategy?", "persona": "Liskov", "domain": "contracts", "category": "cpu_flow", "severity": "LOW", "status": "OPEN"}
{"id": "PH1-Q-060", "text": "The 'add' keyword is a seed for 'task'. But 'add' also appears in common non-task contexts: 'I would add that...', 'additionally...', 'address this...'. Without context windowing, any input containing 'add' gets a task signal. What is the false positive rate for short, ambiguous seed keywords?", "persona": "Turing", "domain": "falsification", "category": "magic_words", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-061", "text": "What is the minimum viable set of seeds for Phase 1 to be useful? Currently there are 49 seeds. If we removed all seeds with ambiguous keywords (add, run, write, send, plan, process, data, content, build, ship, update), we would have 38 seeds. Would the system still achieve a useful CPU hit rate?", "persona": "Knuth", "domain": "algorithms", "category": "northstar", "severity": "MEDIUM", "status": "OPEN"}
{"id": "PH1-Q-062", "text": "The confidence cache (self._confidence_cache) is never bounded. If the learner accumulates millions of patterns over time, the cache grows without limit. Is there a max-size or LRU eviction policy? Memory usage could grow unbounded in long-running sessions.", "persona": "Schneier", "domain": "security", "category": "edge_cases", "severity": "MEDIUM", "status": "OPEN"}
