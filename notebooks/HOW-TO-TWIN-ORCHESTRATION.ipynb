{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1db7412",
   "metadata": {},
   "source": [
    "# HOW-TO-TWIN-ORCHESTRATION\n",
    "\n",
    "Quest: Twin Blade\n",
    "Belt target: Green -> Black\n",
    "\n",
    "Objective:\n",
    "- demonstrate CPU prepass + LLM fallback\n",
    "- verify receipts for every turn\n",
    "- confirm extension-aware skills/persona/identity loading\n",
    "\n",
    "Gate:\n",
    "- CPU routes work (/skills, /recipes, /kernel, /status)\n",
    "- LLM fallback works with reachable endpoint\n",
    "- artifacts/twin contains route + graph + hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "repo = Path.cwd().resolve()\n",
    "while repo != repo.parent and not (repo / 'cli' / 'src').exists():\n",
    "    repo = repo.parent\n",
    "assert (repo / 'cli' / 'src').exists(), 'Repo root not found'\n",
    "\n",
    "env_prefix = 'PYTHONPATH=' + str(repo / 'src/cli/src')\n",
    "\n",
    "cpu_cmd = env_prefix + ' python -m stillwater twin \"/skills\" --json'\n",
    "print('$ ' + cpu_cmd)\n",
    "cpu = subprocess.run(['bash', '-lc', cpu_cmd], cwd=str(repo), capture_output=True, text=True)\n",
    "print(cpu.stdout)\n",
    "if cpu.returncode != 0:\n",
    "    print(cpu.stderr)\n",
    "    raise RuntimeError('CPU twin command failed: ' + str(cpu.returncode))\n",
    "\n",
    "probe_cmd = env_prefix + ' python -m stillwater llm probe-ollama --json'\n",
    "probe = subprocess.run(['bash', '-lc', probe_cmd], cwd=str(repo), capture_output=True, text=True)\n",
    "if probe.returncode != 0:\n",
    "    print(probe.stdout)\n",
    "    print(probe.stderr)\n",
    "    raise RuntimeError('ollama probe failed: ' + str(probe.returncode))\n",
    "probe_data = json.loads(probe.stdout)\n",
    "preferred = probe_data.get('preferred')\n",
    "if preferred:\n",
    "    llm_cmd = env_prefix + ' python -m stillwater twin \"Summarize twin orchestration in 3 bullets.\" --json'\n",
    "    print('$ ' + llm_cmd)\n",
    "    llm = subprocess.run(['bash', '-lc', llm_cmd], cwd=str(repo), capture_output=True, text=True)\n",
    "    print(llm.stdout)\n",
    "    if llm.returncode != 0:\n",
    "        print(llm.stderr)\n",
    "        raise RuntimeError('LLM twin command failed: ' + str(llm.returncode))\n",
    "else:\n",
    "    print('No reachable Ollama endpoint. LLM fallback step skipped.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
