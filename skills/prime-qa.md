<!-- QUICK LOAD (10-15 lines): Use this block for fast context; load full file for production.
SKILL: prime-qa v1.0.0
PURPOSE: Question-first QA discipline. Questions are first-class artifacts. Every audit starts with question formulation, not test execution.
CORE CONTRACT: Questions generated by one agent, answered by a different agent (decoupled verification, CoVe principle). Every GREEN claim requires a falsifier.
GLOW TAXONOMY: G=capability questions, L=learning questions, O=output questions, W=wins questions. Mapped to GLOW dimensions.
STATE MACHINE: QUESTION_GEN → DECOUPLE → ANSWER → SCORE → FALSIFY → REPORT
FORBIDDEN: SELF_CONFIRMED_GREEN | MOCK_AS_EVIDENCE | PROSE_AS_PROOF | UNTESTED_INTEGRATION_CLAIMED | FALSIFIER_SKIPPED
RUNG: 641 = questions formulated + honest scoring | 274177 = falsifiers for every GREEN + adversarial review | 65537 = independent reproduction by separate agent + all falsifiers tested
-->
PRIME_QA_SKILL:
  version: 1.0.0
  profile: fail_closed
  authority: 65537
  northstar: Phuc_Forecast
  objective: Max_Love
  status: FINAL

  # ============================================================
  # PRIME QA — Question-First QA Discipline (v1.0.0)
  #
  # Core insight (from CoVe research, Dhuliawala et al. 2023):
  # Self-confirmation bias is the root cause of most false GREEN
  # verdicts. An agent that generates its own questions and then
  # answers them will systematically confirm its own beliefs.
  #
  # Solution: Decouple question generation from question answering.
  # The qa-questioner generates the hardest possible questions.
  # The qa-scorer answers them against actual project state.
  # Neither agent may read the other's output before completing
  # its own work. The separation is structural, not aspirational.
  #
  # Key principles:
  # - Questions are first-class artifacts, versioned and numbered
  # - Every claim is YELLOW by default until evidence confirms GREEN
  # - Every GREEN claim must have a falsifier (what makes it RED)
  # - Integration boundaries are always probed (cross-project risk)
  # - Fail closed: UNKNOWN beats unjustified GREEN
  # ============================================================

  # ------------------------------------------------------------
  # A) Question Taxonomy (GLOW Dimensions)
  # ------------------------------------------------------------
  Question_Taxonomy:
    # GLOW = Growth, Learning, Output, Wins
    # Each dimension asks a different type of question.
    # The taxonomy maps GLOW to QA question types so every
    # audit covers all four failure modes.

    G_capability:
      dimension: Growth
      definition: "Does the system have the capabilities it claims?"
      question_pattern: "Can the system actually do X? Show the command and output."
      evidence_type: executable_command_output
      scoring_signal: "running the claimed feature produces claimed result"
      example_questions:
        - "Can stillwater/cli actually execute a skill? Show the full command and output."
        - "Does paudio produce deterministic TTS? Run it twice, compare SHA-256."
        - "Does OAuth3 vault actually encrypt? Show the AES-256-GCM key derivation path."

    L_learning:
      dimension: Learning
      definition: "Is new knowledge encoded in durable, retrievable form?"
      question_pattern: "Where is X captured? Show the file path and line number."
      evidence_type: repo_path_plus_line_witness
      scoring_signal: "artifact exists in git at the claimed path with claimed content"
      example_questions:
        - "Where is the OAuth3 token revocation procedure documented? Give file:line."
        - "What skill governs the verification ladder? Show the file that defines rung_65537."
        - "Where is the postmortem for pm-2026-02-21-001? Show the file."

    O_output:
      dimension: Output
      definition: "Are claimed deliverables actually present and verifiable?"
      question_pattern: "Does X exist? Show the git log entry, file hash, and exit code."
      evidence_type: git_artifact_plus_hash
      scoring_signal: "git log shows commit, file is present, hash matches claim"
      example_questions:
        - "Is ROADMAP.md present for each project? Run ls and show the output."
        - "Does tests.json exist for the last major feature? Show the path and sha256."
        - "Is the evidence bundle for the last rung 641 claim complete? List all required files."

    W_wins:
      dimension: Wins
      definition: "Have claimed strategic wins actually been achieved?"
      question_pattern: "Is X measurably true? Show the metric value before and after."
      evidence_type: before_after_metric_with_witness
      scoring_signal: "NORTHSTAR metric has changed; checkpoint exists"
      example_questions:
        - "Has the recipe hit rate actually improved? Show the measurement command and result."
        - "Is the OAuth3 spec published at rung 274177+? Show the evidence bundle path."
        - "Is the Stillwater Store submission live? Show the store URL or commit hash."

  # ------------------------------------------------------------
  # B) Scoring Protocol
  # ------------------------------------------------------------
  Scoring_Protocol:
    verdicts:
      GREEN:
        definition: "Claim is verified with executable evidence."
        requirements:
          - evidence_type_matches_claim: true
          - evidence_is_executable_or_repo_grounded: true
          - falsifier_defined: true   # MANDATORY — no GREEN without falsifier
          - independent_agent_scored: true  # qa-scorer must differ from qa-questioner
        example: "pytest passes, repro_green.log shows exit_code=0, falsifier is 'delete the tested file'"

      YELLOW:
        definition: "Claim is plausible but evidence is weak, partial, or untested."
        triggers:
          - evidence_is_prose_or_plan: true
          - evidence_is_partial_coverage: true
          - integration_boundary_not_tested: true
          - test_uses_mocks_not_real_services: true
        example: "README says feature works, but no test command provided"

      RED:
        definition: "Claim is false, broken, or unverifiable."
        triggers:
          - command_fails: true
          - file_missing_from_git: true
          - test_fails: true
          - integration_probe_fails: true
          - falsifier_triggered: true
        example: "pytest exits 1; feature crashes on real input"

    default_verdict: YELLOW
    # Every claim starts YELLOW. Evidence moves it to GREEN. Failure moves it to RED.
    # Absence of evidence is not GREEN. Absence of evidence is YELLOW at best.

    scoring_discipline:
      - score_from_artifacts_not_memory: true
      - score_from_repo_not_plan: true
      - prose_confidence_never_upgrades_score: true
      - partial_coverage_stays_YELLOW: true
      - mock_as_only_evidence_stays_YELLOW: true

  # ------------------------------------------------------------
  # C) Decoupled Verification Protocol (CoVe Principle)
  # ------------------------------------------------------------
  Decoupled_Verification:
    principle:
      - "Questions are generated by qa-questioner (a separate agent)."
      - "Questions are answered by qa-scorer (a different agent)."
      - "Neither agent may read the other's output before completing its own work."
      - "This prevents self-confirmation bias — the root cause of false GREENs."

    cove_research_basis:
      citation: "Chain-of-Verification Reduces Hallucination in LLMs (Dhuliawala et al. 2023)"
      key_finding: "Models that verify their own outputs against self-generated questions still hallucinate at high rates. Independent verification substantially reduces false positives."

    enforcement:
      questioner_rules:
        - must_not_read_scorer_output_before_generating_questions: true
        - must_generate_falsifying_questions_not_confirming_questions: true
        - must_tag_each_question_with_glow_dimension: true
        - must_specify_expected_evidence_type_per_question: true
      scorer_rules:
        - must_not_read_questioner_reasoning_only_the_question_list: true
        - must_answer_from_actual_repo_state_not_memory: true
        - must_provide_evidence_citation_per_answer: true
        - must_assign_GREEN_YELLOW_RED_per_question: true
        - must_define_falsifier_for_every_GREEN: true

    agent_separation:
      questioner_agent: qa-questioner
      scorer_agent: qa-scorer
      separation_type: structural  # different agents, not just different prompts
      forbidden: SAME_AGENT_GENERATES_AND_SCORES

  # ------------------------------------------------------------
  # D) Falsifier Requirement (Hard Gate)
  # ------------------------------------------------------------
  Falsifier_Requirement:
    definition:
      - "A falsifier is a concrete condition that, if true, would make the GREEN claim RED."
      - "Every GREEN claim must specify its falsifier at the time of scoring."
      - "If no falsifier can be stated, the claim must be scored YELLOW, not GREEN."

    falsifier_schema:
      fields:
        - claim: "The GREEN claim being scored"
        - falsifier: "The condition that would make this claim RED"
        - falsifier_test: "The command or check that would trigger the falsifier"
        - falsifier_status: "UNTESTED | TESTED_DOES_NOT_TRIGGER | TRIGGERED_NOW_RED"

    examples:
      - claim: "stillwater/cli installs without errors"
        falsifier: "pip install fails on a clean Python 3.10 environment"
        falsifier_test: "python -m venv /tmp/test_env && /tmp/test_env/bin/pip install -e ."
        falsifier_status: TESTED_DOES_NOT_TRIGGER

      - claim: "paudio TTS is deterministic"
        falsifier: "Two runs with identical inputs produce different audio SHA-256"
        falsifier_test: "run paudio twice, compare sha256sum of outputs"
        falsifier_status: TESTED_DOES_NOT_TRIGGER

      - claim: "OAuth3 vault encrypts with AES-256-GCM"
        falsifier: "Vault file is plaintext or uses weaker cipher"
        falsifier_test: "hexdump first 32 bytes of vault file; check for AES header"
        falsifier_status: UNTESTED

    rung_requirements:
      rung_641: "Falsifier must be defined for every GREEN claim"
      rung_274177: "Falsifier must be tested (falsifier_status != UNTESTED)"
      rung_65537: "All falsifiers tested by a separate agent from the one that scored GREEN"

  # ------------------------------------------------------------
  # E) Integration Probe Protocol
  # ------------------------------------------------------------
  Integration_Probe_Protocol:
    purpose:
      - "Integration boundaries are where systems fail silently."
      - "A skill that works in isolation may fail when called across project boundaries."
      - "Integration probes test the actual handoff, not the individual components."

    probe_types:
      api_call_probe:
        definition: "Does a real API call succeed across a project boundary?"
        evidence: "curl or requests output with status code and response body"
        example: "Does solaceagi.com /api/v1/tts actually call paudio and return audio?"

      data_handoff_probe:
        definition: "Does the output of A actually feed correctly into B?"
        evidence: "Run A, capture output, pipe to B, check B's exit code and output"
        example: "Does stillwater/cli recipe output parse correctly into solace-cli?"

      auth_boundary_probe:
        definition: "Does authentication work across the integration?"
        evidence: "Token presented to service B, B validates it, authorized response"
        example: "Does OAuth3 vault token grant access to solaceagi.com APIs?"

      failure_propagation_probe:
        definition: "Does failure in A propagate correctly to B (not silently swallowed)?"
        evidence: "Inject error in A, observe that B correctly errors, not silently continues"
        example: "Does paudio TTS failure cause solaceagi.com to return 500, not 200?"

    integration_probe_questions:
      pattern: "When [component A] hands off to [component B], what is the actual behavior?"
      required_evidence: actual_command_output_not_documentation
      YELLOW_triggers:
        - only_documentation_tested: true
        - mocks_used_for_integration: true
        - no_real_service_call_made: true

    ecosystem_boundaries:
      # The 9-project architecture creates these integration boundaries.
      # Each is a required probe surface for any full QA audit.
      stillwater_to_solace_cli: "stillwater/cli → solace-cli skill extension"
      paudio_to_solaceagi: "paudio TTS API → solaceagi.com /api/v1/tts"
      pvideo_to_solaceagi: "pvideo avatar → solaceagi.com avatar system"
      solace_browser_to_oauth3: "solace-browser → OAuth3 token vault"
      solace_cli_to_solaceagi: "solace-cli → solaceagi.com backend auth"

  # ------------------------------------------------------------
  # F) State Machine
  # ------------------------------------------------------------
  State_Machine:
    STATE_SET:
      - INIT
      - INTAKE_SCOPE
      - NULL_CHECK
      - QUESTION_GEN      # qa-questioner generates questions
      - DECOUPLE          # questions handed off; questioner stops
      - ANSWER            # qa-scorer reads questions, reads actual project state
      - SCORE             # qa-scorer assigns GREEN/YELLOW/RED per question
      - FALSIFY           # qa-scorer defines falsifier for every GREEN
      - FALSIFIER_TEST    # qa-scorer tests falsifiers (required for rung 274177+)
      - INTEGRATION_PROBE # cross-project boundary tests
      - REPORT            # gap report + scorecard
      - FINAL_SEAL
      - EXIT_PASS
      - EXIT_NEED_INFO
      - EXIT_BLOCKED

    TRANSITIONS:
      - INIT -> INTAKE_SCOPE: on CNF capsule received
      - INTAKE_SCOPE -> NULL_CHECK: always
      - NULL_CHECK -> EXIT_NEED_INFO: if scope_missing or project_undefined
      - NULL_CHECK -> QUESTION_GEN: if scope_defined

      - QUESTION_GEN -> DECOUPLE: when question_list_complete
      - DECOUPLE -> ANSWER: questions_handed_to_scorer (questioner produces no more output)

      - ANSWER -> SCORE: when all questions have evidence citations
      - SCORE -> EXIT_BLOCKED: if any GREEN claimed without evidence
      - SCORE -> FALSIFY: when all verdicts assigned

      - FALSIFY -> EXIT_BLOCKED: if any GREEN lacks falsifier
      - FALSIFY -> FALSIFIER_TEST: if rung_target >= 274177
      - FALSIFY -> INTEGRATION_PROBE: if rung_target == 641 and integration_probes_in_scope
      - FALSIFY -> REPORT: if rung_target == 641 and no_integration_probes

      - FALSIFIER_TEST -> EXIT_BLOCKED: if any falsifier triggers (claim was wrong)
      - FALSIFIER_TEST -> INTEGRATION_PROBE: if all falsifiers hold
      - INTEGRATION_PROBE -> REPORT: when probes complete

      - REPORT -> FINAL_SEAL: always
      - FINAL_SEAL -> EXIT_PASS: if rung_requirements_met
      - FINAL_SEAL -> EXIT_BLOCKED: if rung_requirements_not_met

    FORBIDDEN_STATES:
      - SELF_CONFIRMED_GREEN: same agent generates and scores questions
      - MOCK_AS_EVIDENCE: mock or stub used as sole evidence for GREEN
      - PROSE_AS_PROOF: prose description used instead of executable evidence
      - UNTESTED_INTEGRATION_CLAIMED: integration works claimed without cross-boundary probe
      - FALSIFIER_SKIPPED: GREEN claim without a defined falsifier
      - QUESTION_BIAS: questioner writes confirming questions, not falsifying questions
      - SCORE_WITHOUT_CITATION: verdict assigned without evidence citation
      - INTEGRATION_OMITTED: cross-project boundaries not probed when in scope

  # ------------------------------------------------------------
  # G) Verification Ladder
  # ------------------------------------------------------------
  Verification_Ladder:
    RUNG_641:
      meaning: "QA questions formulated + honest scoring"
      requires:
        - question_list_numbered_and_tagged_with_glow_dimension: true
        - each_question_has_expected_evidence_type: true
        - each_verdict_has_evidence_citation: true
        - no_GREEN_without_falsifier_defined: true
        - questioner_and_scorer_are_different_agents: true
        - scorecard_produced_with_GREEN_YELLOW_RED_counts: true

    RUNG_274177:
      meaning: "Falsifiers tested + adversarial review"
      requires:
        - all_RUNG_641_requirements: true
        - all_GREEN_falsifiers_tested_not_just_defined: true
        - integration_probes_run_for_claimed_integrations: true
        - adversarial_review_by_third_agent_or_human: true
        - gap_report_identifies_YELLOW_remediation_path: true

    RUNG_65537:
      meaning: "Independent reproduction by separate agent + all falsifiers tested"
      requires:
        - all_RUNG_274177_requirements: true
        - complete_audit_reproduced_by_agent_with_no_prior_context: true
        - all_integration_probes_use_real_services_not_mocks: true
        - behavioral_drift_from_prior_audit_documented: true
        - gap_report_approved_by_human_or_judge_agent: true

  # ------------------------------------------------------------
  # H) Evidence Schema
  # ------------------------------------------------------------
  Evidence:
    required_files:
      - "qa_questions.json"       # question list produced by qa-questioner
      - "qa_scorecard.json"       # per-question verdicts produced by qa-scorer
      - "qa_falsifiers.json"      # falsifier definitions + test status
      - "qa_gap_report.md"        # human-readable gap report
      - "qa_integration_probes.json"  # cross-boundary probe results

    qa_questions_schema:
      required_keys:
        - schema_version
        - generated_by: "agent_id of qa-questioner"
        - scope: "project or feature audited"
        - question_count
        - questions: "list of {id, text, glow_dimension, expected_evidence_type}"

    qa_scorecard_schema:
      required_keys:
        - schema_version
        - scored_by: "agent_id of qa-scorer (must differ from generated_by)"
        - question_ref: "link to qa_questions.json"
        - verdicts: "list of {question_id, verdict, evidence_citation, evidence_type}"
        - summary: "{GREEN_count, YELLOW_count, RED_count}"

    qa_falsifiers_schema:
      required_keys:
        - schema_version
        - question_id
        - claim
        - falsifier
        - falsifier_test
        - falsifier_status: "UNTESTED | TESTED_DOES_NOT_TRIGGER | TRIGGERED_NOW_RED"

    qa_gap_report_schema:
      required_sections:
        - "## Summary (GREEN/YELLOW/RED counts)"
        - "## GREEN Claims (with falsifiers)"
        - "## YELLOW Gaps (with remediation path)"
        - "## RED Failures (with root cause)"
        - "## Integration Probe Results"
        - "## Rung Assessment"

  # ------------------------------------------------------------
  # I) Output Contract
  # ------------------------------------------------------------
  Output_Contract:
    on_pass:
      status: PASS
      include:
        - qa_scorecard.json
        - qa_falsifiers.json
        - qa_gap_report.md
        - rung_achieved
        - GREEN_count
        - YELLOW_count
        - RED_count

    on_blocked:
      status: BLOCKED
      include:
        - stop_reason
        - last_known_state
        - questions_that_triggered_block
        - remediation_steps

    on_need_info:
      status: NEED_INFO
      include:
        - missing_scope_fields
        - what_cannot_be_scored_without_them

    structured_refusal_format:
      required_keys:
        - status: "[NEED_INFO|BLOCKED]"
        - stop_reason
        - last_known_state
        - missing_fields_or_evidence
        - next_actions

  # ------------------------------------------------------------
  # J) Anti-Patterns
  # ------------------------------------------------------------
  Anti_Patterns:
    SELF_CONFIRMED_GREEN:
      symptom: "One agent generates questions and then scores them GREEN"
      fix: "qa-questioner and qa-scorer must be structurally different agents; enforce via agent_id in evidence"

    MOCK_AS_EVIDENCE:
      symptom: "Tests pass because mocks return success, not because real services work"
      fix: "Integration probes must call real endpoints at rung 274177+; mock-only scores are YELLOW"

    PROSE_AS_PROOF:
      symptom: "README or plan document used as evidence for GREEN"
      fix: "Evidence must be executable command output or repo path + line witness; prose = YELLOW"

    UNTESTED_INTEGRATION_CLAIMED:
      symptom: "Agent claims 'paudio integrates with solaceagi.com' without running a real API call"
      fix: "Integration claims require integration_probe with real service; undone probes = YELLOW"

    FALSIFIER_SKIPPED:
      symptom: "Agent assigns GREEN but cannot state what would make it RED"
      fix: "If no falsifier can be stated, score is YELLOW until a falsifier is defined and tested"

    QUESTION_BIAS:
      symptom: "Questioner writes 'Does the feature work as expected?' instead of 'Under what conditions does it fail?'"
      fix: "Questions must seek falsifiers, not confirmations; reframe as 'Show me when X breaks'"

    RUNG_INFLATION:
      symptom: "Agent claims rung 65537 with only prose-confirmed GREENs and no integration probes"
      fix: "Rung is MIN(all contributing agent rungs); integration probes are required for 274177+"
