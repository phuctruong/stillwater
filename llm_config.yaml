# Stillwater OS - LLM Configuration
# Auth: 65537
# Purpose: Centralized LLM endpoint configuration for all notebooks and solvers
#
# Switch between providers by setting `provider:` and updating the URL/API key below

# Current active provider
#
# Default is an offline demo provider so the repo can be executed without any
# external services. Switch to "claude-code" (local wrapper) or an API provider
# when you want LLM-backed runs.
provider: "claude-code"  # Options: "offline", "claude-code", "openai", "claude", "openrouter", "togetherai", "gemini", "ollama"

# Offline Demo - Default
offline:
  name: "Offline (Demo Mode)"
  type: "offline"
  url: ""
  description: "No network calls. Notebooks and demos run deterministically."
  requires_api_key: false
  environment_variables: []

# Claude Code (Local Server) - Default Provider
# Start with: python3 src/claude_code_wrapper.py --port 8080 &
claude-code:
  name: "Claude Code Haiku (Local Server)"
  type: "http"
  url: "http://localhost:8080"
  model: "claude-haiku-4-5-20251001"  # Fast + cheap; change to claude-sonnet-4-6 or claude-opus-4-6
  description: "Local Claude Code wrapper on localhost:8080 (Ollama-compatible API). Default provider."
  requires_api_key: false
  environment_variables:
    - "ANTHROPIC_API_KEY"  # Required to run claude-code server

# Alternative: OpenAI API
openai:
  name: "OpenAI"
  type: "api"
  url: "https://api.openai.com/v1"
  model: "gpt-4o-mini"  # Change to gpt-4 for full GPT-4, gpt-5 for GPT-5 when available
  description: "OpenAI API endpoint"
  requires_api_key: true
  environment_variables:
    - "OPENAI_API_KEY"

# Alternative: Anthropic Claude (Direct API)
claude:
  name: "Anthropic Claude"
  type: "api"
  url: "https://api.anthropic.com/v1"
  model: "claude-haiku-4-5-20251001"  # Or: claude-sonnet-4-6, claude-opus-4-6
  description: "Anthropic Claude API endpoint"
  requires_api_key: true
  environment_variables:
    - "ANTHROPIC_API_KEY"

# Alternative: OpenRouter (Proxy for multiple models)
openrouter:
  name: "OpenRouter"
  type: "api"
  url: "https://openrouter.ai/api/v1"
  model: "openai/gpt-4o"  # Can use any model available on OpenRouter
  description: "OpenRouter proxy - supports 100+ models"
  requires_api_key: true
  environment_variables:
    - "OPENROUTER_API_KEY"

# Alternative: TogetherAI
togetherai:
  name: "TogetherAI"
  type: "api"
  url: "https://api.together.xyz"
  model: "meta-llama/Llama-3-70b-chat-hf"
  description: "TogetherAI endpoint - optimized for open models"
  requires_api_key: true
  environment_variables:
    - "TOGETHER_API_KEY"

# Alternative: Google Gemini
gemini:
  name: "Google Gemini"
  type: "api"
  url: "https://generativelanguage.googleapis.com/v1beta/openai"
  model: "gemini-2-0-pro"
  description: "Google Gemini API endpoint"
  requires_api_key: true
  environment_variables:
    - "GOOGLE_API_KEY"

# Alternative: Ollama (Local, open models)
ollama:
  name: "Ollama (Local)"
  type: "http"
  url: "http://localhost:11434"
  model: "llama3.1:8b"  # Or: mistral, qwen2.5-coder:7b, deepseek-coder:6.7b
  description: "Ollama local server - run open models without API keys"
  requires_api_key: false
  environment_variables: []

# Quick reference table for switching
# ===================================
# To use Claude Code:  provider: "claude-code" (DEFAULT - start wrapper first)
# To use Ollama:       provider: "ollama"     (start ollama serve first)
# To use OpenAI:       provider: "openai"     + export OPENAI_API_KEY=sk-...
# To use Claude:       provider: "claude"     + export ANTHROPIC_API_KEY=sk-ant-...
# To use OpenRouter:   provider: "openrouter" + export OPENROUTER_API_KEY=sk-or-...
# To use TogetherAI:   provider: "togetherai" + export TOGETHER_API_KEY=...
# To use Gemini:       provider: "gemini"     + export GOOGLE_API_KEY=...
# To use Offline:      provider: "offline"    (no network, deterministic demo mode)
